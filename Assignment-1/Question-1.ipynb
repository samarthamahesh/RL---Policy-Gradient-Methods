{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TML Assign-1 (Q1)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIEqI-Etgfi9"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_MVZH7z9MpB"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPNxAf11gmO3"
      },
      "source": [
        "# Initialize parameters and environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVT9zW8T-S_T"
      },
      "source": [
        "params = {\n",
        "    \"discount\": 0.9,\n",
        "    \"tol\": 1e-3,\n",
        "    \"max_iters\": 1000,\n",
        "    \"sim_N\": 20,\n",
        "    \"limit\": 100\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7wutu5x9U7p"
      },
      "source": [
        "env = gym.make('CliffWalking-v0')\n",
        "\n",
        "# Changing the dynamics of terminal state\n",
        "env.P[47] = {\n",
        "    0: [(1.0, 47, 0, True)],\n",
        "    1: [(1.0, 47, 0, True)],\n",
        "    2: [(1.0, 47, 0, True)],\n",
        "    3: [(1.0, 47, 0, True)]\n",
        "}\n",
        "\n",
        "env.P[36][1] = [(1.0, 36, -100, True)]\n",
        "\n",
        "for i in range(25, 35):\n",
        "    env.P[i][2] = [(1.0, 36, -100, True)]\n",
        "\n",
        "for i in range(37, 47):\n",
        "    env.P[i][0] = [(1.0, 36, -100, True)]\n",
        "    env.P[i][1] = [(1.0, 36, -100, True)]\n",
        "    env.P[i][2] = [(1.0, 36, -100, True)]\n",
        "    env.P[i][3] = [(1.0, 36, -100, True)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkP5F_Tt9ZmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f8e191-c8f6-47f6-aa53-e151cdfd4430"
      },
      "source": [
        "nS = env.observation_space.n\n",
        "nA = env.action_space.n\n",
        "\n",
        "print(\"Number of states :\", nS, \"\\nNumber of actions:\", nA)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of states : 48 \n",
            "Number of actions: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7M7mLFHKgET"
      },
      "source": [
        "# DP Agent using Value Iteration algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E50nFZT2EEEh"
      },
      "source": [
        "class DPAgentValueIteration():\n",
        "    '''\n",
        "    Models Dynamic Programming RL agent which is trained using Value Iteration algorithm\n",
        "    '''\n",
        "    def __init__(self, env, params):\n",
        "        '''\n",
        "        Method to initialize the DP agent model\n",
        "\n",
        "        Input\n",
        "        -----\n",
        "        env     : Environment\n",
        "        params  : Parameters for training and simulation\n",
        "        '''\n",
        "        self.env = env\n",
        "        self.nS = env.observation_space.n\n",
        "        self.nA = env.action_space.n\n",
        "        self.params = params\n",
        "        \n",
        "        # Initialize state values\n",
        "        self.V = np.random.rand(self.nS)\n",
        "        self.V[-1] = 0\n",
        "\n",
        "        # Initialize policy\n",
        "        # self.policy = np.random.randint(self.nA, size=self.nS)\n",
        "        self.policy = np.zeros(self.nS)\n",
        "        self.policy[-1] = 0\n",
        "\n",
        "    def value_iteration(self):\n",
        "        '''\n",
        "        Method to train the model using Value Iteration algorithm\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        V       : State values\n",
        "        returns : List with returns after each training iteration\n",
        "        '''\n",
        "        V = np.random.rand(self.nS)\n",
        "        V[-1] = 0\n",
        "\n",
        "        returns = []\n",
        "        iter = 0\n",
        "\n",
        "        while True:\n",
        "            delta = 0\n",
        "\n",
        "            for state in range(self.nS):\n",
        "                v = V[state]\n",
        "                max_val = -100000\n",
        "\n",
        "                for action in range(self.nA):\n",
        "                    val = 0\n",
        "\n",
        "                    for tmp in self.env.P[state][action]:\n",
        "                        val += tmp[0] * (tmp[2] + self.params[\"discount\"] * V[tmp[1]])\n",
        "\n",
        "                    if val > max_val:\n",
        "                        max_val = val\n",
        "\n",
        "                V[state] = max_val\n",
        "                delta = max(delta, abs(V[state] - v))\n",
        "\n",
        "            self.V = V\n",
        "            self.update_policy()\n",
        "\n",
        "            rets = []\n",
        "            for i in range(self.params[\"sim_N\"]):\n",
        "                rets.append(self.simulate())\n",
        "            returns.append(np.mean(rets))\n",
        "            \n",
        "            iter += 1\n",
        "\n",
        "            if delta < self.params[\"tol\"]:\n",
        "                break\n",
        "\n",
        "        print(\"Number of iterations taken to converge:\", iter, \"\\n\")\n",
        "        \n",
        "        return V, returns\n",
        "\n",
        "    def train_agent(self):\n",
        "        '''\n",
        "        Method to train agent\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        returns: A list with returns after each training iteration\n",
        "        '''\n",
        "        self.V, returns = self.value_iteration()\n",
        "        self.update_policy()\n",
        "        return returns\n",
        "\n",
        "    def update_policy(self):\n",
        "        '''\n",
        "        Method to update policy\n",
        "        '''\n",
        "        for state in range(self.nS):\n",
        "            max_value = -1000000\n",
        "            max_action = -1\n",
        "\n",
        "            for action in range(self.nA):\n",
        "                val = 0\n",
        "\n",
        "                for tmp in self.env.P[state][action]:\n",
        "                    val += tmp[0] * (tmp[2] + self.params[\"discount\"] * self.V[tmp[1]])\n",
        "                \n",
        "                if val > max_value:\n",
        "                    max_value = val\n",
        "                    max_action = action\n",
        "\n",
        "            self.policy[state] = max_action\n",
        "\n",
        "    def simulate(self):\n",
        "        '''\n",
        "        Method to run the simulation using current policy\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        ret: Return of the simulation\n",
        "        '''\n",
        "        ret = 0\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        iter = 0\n",
        "\n",
        "        while not done and iter < self.params[\"max_iters\"]:\n",
        "            if random.uniform(0, 1) < 0.1:\n",
        "                action = self.env.action_space.sample()\n",
        "            else:\n",
        "                action = self.policy[state]\n",
        "\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            ret += reward\n",
        "\n",
        "            state = obs\n",
        "            iter += 1\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def get_nS(self):\n",
        "        '''\n",
        "        Getter method for number of states\n",
        "        '''\n",
        "        return self.nS\n",
        "\n",
        "    def get_nA(self):\n",
        "        '''\n",
        "        Getter method for number of actions\n",
        "        '''\n",
        "        return self.nA\n",
        "\n",
        "    def get_policy_state(self, state):\n",
        "        '''\n",
        "        Getter method for policy of the input state\n",
        "        '''\n",
        "        return self.policy[state]\n",
        "\n",
        "    def get_state_value(self, state):\n",
        "        '''\n",
        "        Getter method for state value of the input state\n",
        "        '''\n",
        "        return self.V[state]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "VgRCBEBjwaY3",
        "outputId": "72b73f1d-c4c5-45f0-8bb9-4f4664c76432"
      },
      "source": [
        "# Initialize agent\n",
        "vi_agent = DPAgentValueIteration(env, params)\n",
        "\n",
        "# Train the agent and get returns list\n",
        "returns = vi_agent.train_agent()\n",
        "\n",
        "# Plot the returns vs iterations plot\n",
        "len1 = len(returns)\n",
        "x = np.arange(len1)+1\n",
        "plt.plot(x, returns)\n",
        "\n",
        "min_ret, max_ret, mean_ret = np.min(returns), np.max(returns), np.mean(returns)\n",
        "plt.plot(x, np.ones(len1)*min_ret, label=\"min\")\n",
        "plt.plot(x, np.ones(len1)*max_ret, label=\"max\")\n",
        "plt.plot(x, np.ones(len1)*mean_ret, label=\"mean\")\n",
        "plt.legend()\n",
        "plt.title(\"Rewards vs Iterations\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Rewards\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations taken to converge: 16 \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8dcnCSHsYQvKJqBA2EUBFURBca0Xva22trYVrbW1bvXWvbXXttetetvb7dfWKmK11bpLWzdAAZdaQFkCEhZZE5WEJWEJIdvn98c5wYgJGWBmziS8n4/HPDJzzplzPpnAvOd8v3O+X3N3REREYpEWdQEiItJ0KDRERCRmCg0REYmZQkNERGKm0BARkZgpNEREJGYKDZH9MLMpZvZW1HVExcxuN7OHoq5DUodCQyJjZuvMbLeZ7TSzT8xsmpm1jbquVLFvYIWv16QEHm+CmRXUXebud7v7FYk6pjQ9Cg2J2n+4e1vgWGAkcFtUhZhZRlTHTjQL6P+7HDL9I5KU4O6fAK8ShAcAZnaimb1jZiVmttjMJoTLJ5pZXp3tZpjZ/DqP3zSzC8L7t5rZh2a2w8w+MLP/rLPdFDN728x+aWZbgDvNrLOZTTez7WY2Dzi6zvYWblsUrs8zs6H7/i5m9hUzW7DPshvMbHp4/9ywlh1mVmhmNzb2+pjZY0Bv4O/hmdnN+3uNwnWzzewuM3sbKAP6mdllZrY8PPYaM/tOuG0b4GWge7j/nWbW3czuNLPH6+xzspktC48328wG1Vm3zsxuNLMlZlZqZn8zs6xwXRcz+0f4vK3h30jvP02Ru+umWyQ3YB0wKbzfE8gDfhU+7gFsAc4l+HBzRvi4K9AKKAe6AC2ATUAh0C5ctxvoHO7nIqB7uI+vALuAI8N1U4Aq4FogI3zuk8BTQBtgaLjft8LtzwLeA7IBAwbV7muf36s1sAPoX2fZfODi8P7HwPjwfkfguAZenym1x9739WrsNQrXzwY2AEPC368F8AWCIDTgVIIwOS7cfgJQsE8NdwKPh/cHhK/fGeG+bgZWA5l16psXvt6dgOXAd8N19wB/CJ/XAhgPWNT/BnU78JuSXqL2gpntADYCRcB/h8u/Drzk7i+5e427zwAWAOe6+26CN+FTgOOBxcDbwDjgRGCVu28BcPen3f2jcB9/A1YBY+oc/yN3/427VwEVwJeAH7v7LndfCjxaZ9tKgmDKJXjDW+7uH+/7C7l7GfAi8FUAM+sfPmd6nf0MNrP27r7N3d8/yNeuwdeozjbT3H2Zu1e5e6W7/9PdP/TAHOA1gjfwWHwF+Ke7z3D3SuABgqAdW2ebX4ev91bg73x65lgJHAkcFdbxprtr4LsmSKEhUbvA3dsRfMrNJTh7ADgKuChszigxsxLgZII3HoA54XNOCe/PJvjkfGr4GAAz+6aZLaqzj6F1jgFBWNXqSvCJvO6y9bV33P114LfA74AiM3vQzNo38Hv9lTA0gK8BL4RhAkEwnQusN7M5ZnZSA/toTGOvEfv8LpjZOWb2bthEVBLWUff12J/ufPb1qAn336PONp/UuV8G1H6x4X6Cs5LXwmaxW2M8pqQYhYakhPBT7zSCT68QvBk95u7ZdW5t3P3ecP2+oTGHfULDzI4C/gRcQ9BclQ0sJWia2XvoOveLCZqretVZ1nufOn/t7scDgwmaa25q4FeaAXQ1s2MJwuOvdfYx393PB3KAFwiaw2Kx7yfzxl6jzzzHzFoCzxK8xt3C1+MlPn09Gvvk/xFBUNXuzwheq8JGC3ff4e4/cPd+wGTgv8zs9MaeJ6lHoSGp5P+AM8xsBPA48B9mdpaZpZtZVviV0J7htu8AAwmamua5+zKCN7QTgLnhNm0I3giLAczsMoIzjXq5ezXwHEGHeGszGwxcWrvezEab2Qlm1oKgbb8cqGlgX5XA0wSfsDsRhAhmlmlml5hZh3Cb7Q3tox6bgH51Hjf2Gu0rE2hJGI5mdg5w5j7772xmHRp4/lPAF8zs9PA1+AGwh+BvsV9mdp6ZHRMGTSlQTey/t6QQhYakDHcvBv5M0KewETgfuJ3gTW4jwaf6tHDbXcD7wDJ3rwh38S9gvbsXhdt8APxvuHwTMIyg72N/riFoUvmE4MznkTrr2hOcuWwjaKbZQhAKDfkrMAl4OuwzqfUNYJ2ZbQe+C1zSSE217gF+FDZF3djYa7Qvd98BXEfw5r+NoNlsep31+cATwJrwGN33ef4Kgn6U3wCbgf8g+Mp0BY3rD8wEdhL8Pf6fu78R4+8tKcTUFyUiIrHSmYaIiMRMoSEiIjFTaIiISMwUGiIiErNmO0AbQJcuXbxPnz5RlyEi0qS89957m929a33rmnVo9OnThwULFjS+oYiI7GVm6xtap+YpERGJWZMLDTM728xWmNlqjV8jIpJcTSo0zCydYLC4cwjG/vlqONSDiIgkQZMKDYJxhla7+5pw6IInCYZREBGRJGhqodGDzw71XMBnh2XGzK40swVmtqC4uDipxYmINHdNLTQa5e4Puvsodx/VtWu93xgTEZGD1NRCo5DPznXQkxjG8hcRkfhoatdpzAf6m1lfgrC4mGB457i7b9595G/NT8SuReQwUl3j1LhTXRPe6tyvqYFqd2pqnFaZ6XRo1YL0NGt8pzHI7ZTLLWNuicu+6mpSoeHuVWZ2DfAqkA5MDSffEZFmYtP2cqqqnew2LWiTmVpvUe6wo7ySHXuqqKr2vW/4nw2CTx/XuHMgs0+kmdG+VQuyW7egY+tMWmakXmNQs55PY9SoUa4rwkWajsUbSzj/d5/Ok3VE+ywm5uZwem4OY4/pTOsIQmTT9nJmryjijfxi3l69mR17gvm0WrVIp03LDNq2TKd1ZgZtW2bQpmXtsoxwWfD402Xp4XZ1lrVMJzM9jYUbSpi1fBOv5xexZvMuAAZ0a8tpud2YNCiHkb07xu0spDFm9p67j6p3nUJDRFKBu/PVP73Lqk07efaqscxft5XX84uYu7KYXRXVZGakMfbozpyWm8PEgTn06tQ6IXVUVdewcGPJ3qD44OPtABzZIYsJA7syYWAO447pQtuWiQuwNcU7eT2/iFnLi5i/bitVNU526xZMHJjDabk5nDKgKx1atUjY8RUaIpLy3sgv4rJp8/nJ5CFcOrbP3uUVVTXMX7eVWcuLeD1/E+u2lAGffgo/fVAOI3tlk5F+8E05xTv2MGdlMbNXBCG1vbyK9DTj+KM6MnFgDhNzuzKwWzuCKc6Tq3R3JW+uKub15UW8saKIbWWVpKcZo/t0ZNKgbpyWm0O/rm3jekyFhoiktOoa55xfzaWiqobXbjiVzP205dd+Cn89v4h5a4NP4R1atWDCwK6clpvDqQO6kt06s9HjLS4oYfaKICiWFJQC0LVdSyYM6MrE3OBsIpGf5g9GdY2zaOM2Zi0PzkJWbNoBQN8ubTgtN4fTB+Uwuk8nWhxCgIJCI+oyRKQRT83fyM3PLuH/XXIc5w47MubnbS+v5M2Vm3k9P/gUvnVXBWkGo47qFPSFDMqhf05bzIxtuyqYu6qYN/KLmLOymG1llaQZjOzdkYlhs9PgI9uTlqR+g3jYuLWMN1YEAfKvD7dQUV1Du5YZnDKwK2cO7sb5x/ZofCf1UGiISMraXVHNhAfe4MgOrXj+e2MPugmo9uzh9eXBWUhtX0SP7FZ0adeSJQUluEOnNplMGNCVUwd25ZT+XenYZv9nJU3Frj1VvLV6c/D7ryji6K5tePLKkw5qXwoNEUlZv3tjNfe/uoKnvnMSY/p2itt+Py7dzRv5xbyev4mtuyoY3z9odhrWo0PSvoUUlZoaZ1tZBZ3btjyo5+8vNFLrS9AicljZsnMPv5/9IZMGdYtrYAAc2aEVXzuhN187oXdc99sUpKXZQQdGo/tOyF5FRGLwm9dXU1ZRxa3nDIy6FImRQkNEIrF+yy7+8u/1fGV0b47JaRd1ORIjhYaIROL+V1eQkZbGDZP6R12KHACFhogk3aKNJfxjycd8e3xfctpnRV2OHACFhogklbtzz0vL6dwmkytPPTrqcuQAKTREJKneWFHEv9du5fuT+id0/CZJDIWGiCRNVXUN97yUT98ubbh4zOH3VdjmQKEhIknz7PsFrCrayc1nDTzk8ZEkGvqriUhS7K6o5hczVjKydzZnDz0i6nLkICk0RCQppr69lk3b93D7uYMiGWJc4kOhISIJVztcyBmDuzG6T3yHC5HkUmiISML95vXV7K6s5pazc6MuRQ6RQkNEEmrd5l08/u56vjK6F8fkxHeGOUk+hYaIJNT9r60gMyON72u4kGZBoSEiCbNwwzb+ueRjrhjfj5x2Gi6kOVBoiEhCuDv3vJxPl7aZXHlKv6jLkThRaIhIQsxaXsS8tVu5ftIADRfSjCg0RCTuqqpruO+VfPp1acPFo3tFXY7EkUJDROLumffC4ULO1nAhzY3+miISV2UVVfxixkqO653NWUM0XEhzo9AQkbia+tZainZouJDmSqEhInGzeece/jBnDWcN6cYoDRfSLCk0RCRufjNrFbsrq7lZw4U0WwoNEYmLtZt38Zd/b+Di0b04uquGC2mu9OVpkRRXWV3D7spqyiuq2V0Z3sL75ZXV7K6o2bt8323K69k+Pc0Y2r0Dx/bOZkTPbHp2bBWXvocHXg2GC7lew4U0awoNkRRVXlnNdx57jzkriw/4uZnpaWS1SKNVZjqtWqST1SJ97/3dldU89u56HnprLQBd2mYyomc2x/bK5tje2QzvmU2HVi0O6HgLN2zjn3kf8/1J/TVcSDOn0BBJQdU1zg1/W8SclcV8e3xfurXP2vum36pFOll17u8bDFkZaWQ0cm1EZXUNKz7ZwcKNJSzaUMLighJm5RftXd+vaxuO7RmEyLG9ssk9oj2ZGfXv092556V8urRtybfHa7iQ5k6hIZJi3J2f/n0ZLy/9hB99YRBXJOCNuEV6GkN7dGBojw5848SjANheXsmSjaUsLihh4YYS5q7azHMLCwHIzEhjSPf2jOiZzcgwSHp3ao2ZMXN5EfPWbeV/LhhKGw0X0uxF8hc2s/uB/wAqgA+By9y9JFx3G/AtoBq4zt1fDZefDfwKSAcecvd7o6hdJNH+MGcNj/5rPd8e3zchgdGQ9lktOLl/F07u3wUIwuuj0nIWbShh0cZtLN5YypPzNzDtnXUAdGzdghG9slldtJN+XdvwFQ0XcliI6mPBDOA2d68ys/uA24BbzGwwcDEwBOgOzDSzAeFzfgecARQA881surt/EEHtIgnz3PsF3PdKPpNHdOe2cwZFWouZ0SO7FT2yW/GF4UcCwZhSKzbtYPHGUhZt3MaijSV8UlrOg988XsOFHCYiCQ13f63Ow3eBC8P75wNPuvseYK2ZrQbGhOtWu/saADN7MtxWoSHNxtyVxdz8zBLGHt2Z+y8aTlpa6l1NnZGexpDuHRjSvQNfO6E3EJyR6Mrvw0cqfDS4HHg5vN8D2FhnXUG4rKHln2NmV5rZAjNbUFx84N86EYnC0sJSrnr8PY7JacsfvnE8LTPSoy4pZgqMw0vCzjTMbCZQ32hlP3T3F8NtfghUAX+J13Hd/UHgQYBRo0Z5vPYrkigbtpQx5ZH5ZLfO5NHLx9A+68C+7iqSTAkLDXeftL/1ZjYFOA843d1r39wLgbq9aT3DZexnuUiTtWXnHi59ZB6V1TU8eeUJdGuvaxwktUXSPBV+E+pmYLK7l9VZNR242MxamllfoD8wD5gP9DezvmaWSdBZPj3ZdYvEU1lFFZc/uoCPSnbz8KWjOCanXdQliTQqqm9P/RZoCcwI20PfdffvuvsyM3uKoIO7Crja3asBzOwa4FWCr9xOdfdl0ZQucuiqqmu49q8LySso4fdfP14jwkqTEdW3p47Zz7q7gLvqWf4S8FIi6xJJBnfnRy8sZVZ+Ef9zwVBNVCRNSip8e0rksPJ/M1fx5PyNXDPxGL4eXo0t0lQoNESS6Il5G/jVrFVceHxPfnDmgMafIJJiFBoiSTLzg0388Pk8Jgzsyj1fHKbrG6RJUmiIJMH7G7ZxzRPvM7RHB373teM05IY0WfqXK5JgHxbv5FvT5tOtfRZTp4zWSLDSpCk0RBKoaHs5l06dR5oZj142hi5tW0Zdksgh0UcekQTZUV7JlEfms2VnBU9eeSJ9urSJuiSRQ6bQEEmAiqoarnr8fVZs2sFDl45iRK/sqEsSiQs1T4nEWU2Nc/Mzi3lr9Wbu/eIwJg7MibokkbhRaIjE2X2v5vPCoo+48cwBXDRKs9lJ86LQEImjR95eyx/nrOGSE3pz9cQGR8sRabLUpyESB+WV1dz1z+U89u56zhjcjZ+eP1QX70mzpNAQOUTLP97OdU8sZFXRTq44uS83nT2Q9BScqlUkHhQaIgeppsZ55J113PdyPh1at+DPl4/hlAFdoy5LJKEUGiIHoXjHHm58ejFzVhZzem4OP79wOJ114Z4cBhQaIgfo9fxN3PT0EnbuqeJn5w/h6ycepf4LOWwoNERiVF5ZzT0vLefRf60n94h2PHHliQzopila5fCi0GjAJ3ffzZ7l+VGXISmirKKa1UU7GVxRxaMdWtG7oBVp7xnroy5MpAEtB+VyxO23x32/Cg2R/XBg0/Zy1m8pIyPNyD2yPdmtWkRdlkhkFBoNSERCS9Oyeecebnp6MW+sKGbiwK7cf9EIjVIrhz2Fhkg93lhRxE1PL2Z7eRU/mTyEb56kzm4RUGhIHYs2llBSVkF1jVNV43V+1lBV7Z9bXlVd8/ntapzq6uBxjTun5eYwoQkN2FdeWc19r+TzyNvrGNitHY9fcQK5R7SPuiyRlKHQEAAWbyzhgt+9fUj7yEgz0tNs78/qGufP/1rPRcf35EfnDaZDivcFrNy0g+ueWEj+JzuYMrYPt56TS1aL9KjLEkkpCg0BgjmsAR65bDSdWmcGb/7ptQGQRkb4OAiFtM+EQ+3PfZtv9lRV85tZq/n9nA95c9Vm7v3SsJQ863B3Hnt3PXf9czntsjJ4ZMpoJuamXp0iqUChIQDkFZTStV3LuM790DIjnRvPGsgZg7tx49OLmfLIfC4e3YsffmEQ7bJS46xjy8493PzMEmblF3HqgK48cNEIurZTZ7dIQxQaAkBeYSnDe3RIyL5H9Mrm79eezK9mreKPcz5k7spi7rtwOOP7RzdO056qap57v5BfzFhJaVklPz5vMFPG9iFNAw2K7Jfm0xDKKqr4sHgnQxMUGgBZLdK55excnr1qLK0y0/nGw/O4/fk8du6pStgx67O7oppH3l7LhPtnc9tzeXTvkMWL14zj8pP7KjBEYqAzDeGDj7ZT4zAsgaFRa2TvjvzzuvH8YsZK/vTmGuasKOb+C4cz9pguCT3ujvJKHn93Aw+/tYbNOysY06cT931pOOP7d9FXaUUOgEJDWFJQCsCwnokPDQjOOm4/dxBnDenGjU8v4WsP/ZtvnHgUt56TS5uW8f0nWVJWwdS31zHt7bVsL6/ilAFduWbiMYzp2ymuxxE5XCg0hKWFpeS0a0m39llJPe7xR3XipevG88BrK5j69lpmryzi/gtHcGK/zoe876Id5Tz85loef3c9uyqqOXNwN66eeAwjemXHoXJpTiorKykoKKC8vDzqUpIuKyuLnj170qJF7F9MUWgIeYWlSWmaqk+rzHTuOG8wZw05gpueWczFD77LlLF9uPnsgbTOPPB/noUlu3lwzoc8OX8jldU1nDe8O9+beLQu0JMGFRQU0K5dO/r06XNYNVW6O1u2bKGgoIC+ffvG/DyFxmFu154qVhfv5AvDj4y0jjF9O/Hy9eP5+SsrmPbOOt5YUcQDF41gdJ/YmpHWbt7F72ev5rn3CwH40nE9+e6Eo+nbpU0iy5ZmoLy8/LALDAAzo3PnzhQXFx/Q8xQah7kPPt6OJ6kTvDGtMzO4c/IQzhpyBDc/u5gv//FfXD6uLzeeOZBWmfVfmb3ikx387o3V/GPJR2Skp3HJCb258tSj6ZHdKsnVS1N2uAVGrYP5vRUah7m82k7wFAiNWicd3ZlXrj+Fe1/O5+G31vJGfhH3XzSC44/quHebJQUl/Pb11bz2wSZaZ6bz7fH9+Nb4vuS0S26/jMjhRqFxmMsrLKVb+5bkJLkTvDFtWmbwswuGcs7QI7jpmSVc9Id3uGJ8PyYM6Mof5q5h7spi2mdlcN3p/blsbB86tsmMumSRhJo+fToffPABt956a6R1xBQaZnY98AiwA3gIGAnc6u6vJbA2SYIoO8FjMfaYLrx6wync/dJyHpy7hgfnrqFzm0xuPnsg3zjxqJQZjkQk0SZPnszkyZOjLiPmK8Ivd/ftwJlAR+AbwL2HenAz+4GZuZl1CR+bmf3azFab2RIzO67Otpea2arwdumhHluCTvBEXwkeD21bZnD3fw7jyStP5P4Lh/PWLafxvQnHKDCk2Vi3bh25ublMmTKFAQMGcMkllzBz5kzGjRtH//79mTdvHtOmTeOaa64BYMqUKVx33XWMHTuWfv368cwzzySt1libp2p7S84FHnP3ZXaIPUdm1osghDbUWXwO0D+8nQD8HjjBzDoB/w2MIpiB8z0zm+7u2w6lhsPdso9SpxM8Fif26xyXazhEGvTyrfBJXnz3ecQwOKfxz9irV6/m6aefZurUqYwePZq//vWvvPXWW0yfPp27776bCy644DPbf/zxx7z11lvk5+czefJkLrzwwvjW3YBYzzTeM7PXCELjVTNrB9Qc4rF/CdxMEAK1zgf+7IF3gWwzOxI4C5jh7lvDoJgBnH2Ixz/s5RWmXie4yOGqb9++DBs2jLS0NIYMGcLpp5+OmTFs2DDWrVv3ue0vuOAC0tLSGDx4MJs2bUpanbGeaXwLOBZY4+5lZtYZuOxgD2pm5wOF7r54nxOWHsDGOo8LwmUNLa9v31cCVwL07t37YEs8LCxN0U5wkcjEcEaQKC1bfjokf1pa2t7HaWlpVFV9fmDPutu7++fWJ8p+Q6Nun0KoX6ytUmY2EziinlU/BG4naJqKO3d/EHgQYNSoUcl7JZugJQUlOssQkQPS2JnG/4Y/s4DjgSUE/RvDgQXASQ090d0n1bfczIYBfYHas4yewPtmNgYoBHrV2bxnuKwQmLDP8tmN1C77sXNPFWs272LyiHpP2ERE6rXf0HD3iQBm9hxwvLvnhY+HAncezAHDfeydHs7M1gGj3H2zmU0HrjGzJwk6wkvd/WMzexW428xqr+46E7jtYI4vgQ9qO8F7akwmkaj16dOHpUuX7n08bdq0etdNmTLlc+sBdu7cmegS94q1T2NgbWAAuPtSMxuUgHpeIuhsXw2UEfabuPtWM/sZMD/c7qfuvjUBxz9sLCkoAUj5r9uKSGqJNTTyzOwh4PHw8SUETVWHzN371LnvwNUNbDcVmBqPY0rQCX5E+ywNuyEiByTW0JgCXAVcHz6eS3ANhTRReYWlOssQkQPWaGiYWTrwcti/8cvElySJpk5wETlYjV7c5+7VQI2Z6WNpM7GssBR3GJ6k6V1FpPmItXlqJ0G/xgxgV+1Cd78uIVVJQtVeCa7mKRE5ULGGxnPhTZqBvLATvGu7lo1vLCJSR0yh4e6PJroQSZ68wlKGqWlKRA5CTAMWmll/M3vGzD4wszW1t0QXJ/G3o7yStZt3afgQkRQSy9Do8+bN46STTmLkyJGMHTuWFStWAPDLX/6Syy+/HIC8vDyGDh1KWVlZwmqNtXnqEYKhyX8JTCS46C7WEXIlhTS14dBFkum+efeRvzU/rvvM7ZTLLWNuaXS7xoZG//Of/8ybb75JRkYGM2fO5Pbbb+fZZ5/l+uuvZ8KECTz//PPcdddd/PGPf6R169Zx/R3qijU0Wrn7LDMzd18P3Glm7wE/TlhlkhBL1QkukpJqh0YH6h0avbS0lEsvvZRVq1ZhZlRWVgLBKLjTpk1j+PDhfOc732HcuHEJrTPW0NhjZmnAKjO7hmAAwbaJK0sSJa+wlCM7qBNcpD6xnBEkSmNDo99xxx1MnDiR559/nnXr1jFhwoS9269atYq2bdvy0UcfJbzOWJuYrgdaA9cRjHb7dUBTrjZBeQW6ElykKSotLaVHj+CC3LoDFpaWlnLdddcxd+5ctmzZkvCpX2MNja3uvtPdC9z9Mnf/UjiznjQhO8orWbN5F8MVGiJNzs0338xtt93GyJEjPzMp0w033MDVV1/NgAEDePjhh7n11lspKipKWB0Wy4xPZjaHYA6L+cCbwNy6o96mqlGjRvmCBQuiLiNlvLtmCxc/+C6PXDaaiQNzGn+CyGFg+fLlDBqUiEG7m4b6fn8ze8/dR9W3fazXaZxqZpnAaILJkP5pZm3dvdMh1itJlFegOcFF5NDEFBpmdjIwPrxlA/8gOOOQJiSvsJTuHbLo0lad4CJycGL99tRs4D3gHuAld69IWEWSMEs1HLqIHKJYO8K7AD8lmBP8FTObGc6kJ03E9rATXE1TInIoYu3TKAmHDelF0CE+FmiRyMIkvpYVbgfQmFMickhi7dNYA+QDbxHM2HeZmqialtorwXWmISKHItY+jWPcvSahlUhCLQk7wTurE1xEDkGsfRrHmNksM1sKYGbDzexHCaxL4kyd4CISD7GGxp+A24BKAHdfAlycqKIkvraHw6FreleR1BTL0Oi7du3i8ssvZ8yYMYwcOZIXX3xx73PHjx/Pcccdx3HHHcc777wDwOzZs5kwYQIXXnghubm5XHLJJcRyMXdjYm2eau3u88ys7rKqhjaW1FLbCa4zDZH9++Tuu9mzPL5Do7cclMsRt9/e6HaNDY0+ePBgTjvtNKZOnUpJSQljxoxh0qRJ5OTkMGPGDLKysli1ahVf/epXqR0JY+HChSxbtozu3bszbtw43n77bU4++eRD+n1iDY3NZnY04ABmdiHw8SEdWZImr7AEUCe4SCprbGj0goICpk+fzgMPPABAeXk5GzZsoHv37lxzzTUsWrSI9PR0Vq5cuXefY8aMoWfPngAce+yxrFu3LmmhcTXwIJBrZoXAWuCSQzqyJE1e4XZ6ZLdSJ7hII2I5I0iUxoZGT09P59lnn2XgwIGfed6dd95Jt27dWLx4MTU1NWRlZdW7z/T09M8MdE01cfoAABAJSURBVHiwYurTcPc17j4J6ArkAqcChxZXkjRBJ3j7qMsQkUNw1lln8Zvf/GZvv8TChQuBYGj0I488krS0NB577DGqq6sTWsd+Q8PM2pvZbWb2WzM7AygjmEdjNfDlhFYmcbFdc4KLNAt33HEHlZWVDB8+nCFDhnDHHXcA8L3vfY9HH32UESNGkJ+fT5s2bRJax36HRjezF4FtwL+A04EcwIDr3X1RQiuLAw2NDu98uJmv/enfPHr5GE4d0DXqckRSjoZGj+/Q6P3cfVi4k4cIOr97u3t5PIqVxNOV4CIST431aVTW3nH3aqBAgdG0LCkopUd2Kzq1yYy6FBFpBho70xhhZtvD+wa0Ch8b4O6u3tUUt7SwVGcZIo1wd/a5Du2wcDAX++33TMPd0929fXhr5+4Zde4rMFJc6e5K1m0p08i2IvuRlZXFli1b4nK1dFPi7mzZsuUzX9GNRazXaUgTtCzsz9CV4CIN69mzJwUFBRQXF0ddStJlZWXtvfgvVgqNZixPneAijWrRogV9+/aNuowmI9YBC6UJyitUJ7iIxJdCoxnLUye4iMRZZKFhZteaWb6ZLTOzn9dZfpuZrTazFWZ2Vp3lZ4fLVpvZrdFU3XSU7q5kvTrBRSTOIunTMLOJwPnACHffY2Y54fLBBPN0DAG6AzPNbED4tN8BZwAFwHwzm+7uHyS/+qZhmfozRCQBouoIvwq41933ALh7Ubj8fODJcPlaM1sNjAnXrXb3NQBm9mS4rUKjAUsUGiKSAFE1Tw0AxpvZv81sjpmNDpf3ADbW2a4gXNbQ8s8xsyvNbIGZLTgcv0JXK6+wlJ4dW9FRneAiEkcJO9Mws5nAEfWs+mF43E7AicBo4Ckz6xeP47r7gwRzfzBq1KjD62qdOnQluIgkQsJCI5x/o15mdhXwnAeXYM4zsxqgC1AI9Kqzac9wGftZLvsoLQs6wb88qlfjG4uIHIComqdeACYChB3dmcBmYDpwsZm1NLO+QH9gHjAf6G9mfc0sk6CzfHoklTcBSz8K+jOG65tTIhJnUXWETwWmmtlSoAK4NDzrWGZmTxF0cFcBV4ej62Jm1wCvAunAVHdfFk3pqa/2SvCh3RUaIhJfkYSGu1cAX29g3V3AXfUsfwl4KcGlNQt5BeoEF5HE0BXhzVBeYamapkQkIRQazUxpWSUbtpZpZFsRSQiFRjOjkW1FJJEUGs2MOsFFJJEUGs3M0sJSenVSJ7iIJIZCo5lZUliipikRSRiFRjNSUlbBxq271QkuIgmj0GhGlhZuB2B4j+yIKxGR5kqh0YwsKSwBYGiP9hFXIiLNlUKjGantBM9urU5wEUkMhUYzkldYqqYpEUkohUYzsW2XOsFFJPEUGs1E7XDo+rqtiCSSQqOZ0PAhIpIMCo1mYmlhKb07taZD6xZRlyIizZhCo5lYUqA5wUUk8RQazcC2XRUUbNvNMM2hISIJptBoBtQJLiLJotBoBpYUaDh0EUkOhUYzsLSwlKM6qxNcRBJPodEM5BWW6qI+EUkKhUYTt7cTXKEhIkmg0Gjiai/qG67QEJEkUGg0cbWhMUShISJJoNBo4vIKwk7wVuoEF5HEU2g0cXmFuhJcRJJHodGEbd1VQWGJOsFFJHkUGk2YRrYVkWRTaDRhS9UJLiJJptBowvIKSumjTnARSSKFRhOmK8FFJNkUGk2UOsFFJAoKjSZqbye45tAQkSRSaDRBm7aX84fZH2IGQzQcuogkUUbUBUjs3J1n3y/kp39fxp6qGn52/lB1gotIUkVypmFmx5rZu2a2yMwWmNmYcLmZ2a/NbLWZLTGz4+o851IzWxXeLo2i7iht2l7Otx5dwI1PL2ZAt3a88v1T+PqJR0VdlogcZqI60/g58BN3f9nMzg0fTwDOAfqHtxOA3wMnmFkn4L+BUYAD75nZdHffFkXxyeTuPPd+IT8Jzy5+9IVBXDauL+lpFnVpInIYiio0HGgf3u8AfBTePx/4s7s78K6ZZZvZkQSBMsPdtwKY2QzgbOCJpFadZJu2l3P7c3nMyi9i1FEduf+iEfTt0ibqskTkMBZVaHwfeNXMHiBoIhsbLu8BbKyzXUG4rKHln2NmVwJXAvTu3Tu+VSeJzi5EJFUlLDTMbCZwRD2rfgicDtzg7s+a2ZeBh4FJ8Tiuuz8IPAgwatQoj8c+k0lnFyKSyhIWGu7eYAiY2Z+B68OHTwMPhfcLgV51Nu0ZLiskaKKqu3x2nEpNCe7O8wsLuXO6zi5EJHVFdZ3GR8Cp4f3TgFXh/enAN8NvUZ0IlLr7x8CrwJlm1tHMOgJnhsuahU3by7ni0QX811OL6d+tHS9fP54rxvdTYIhIyomqT+PbwK/MLAMoJ+yDAF4CzgVWA2XAZQDuvtXMfgbMD7f7aW2neFOmswsRaWoiCQ13fws4vp7lDlzdwHOmAlMTXFrSFG0v5/bn85i5vIjjj+rI/RcOp1/XtlGXJSKyX7oiPMncnRcWFXLn9A8or6zW2YWINCkKjSQKzi6WMnP5Jp1diEiTpNBIkhcXFfLjF5fp7EJEmjSFRoLt3FPFj19YynMLCzmudzb3XzSCo3V2ISJNlEIjgRZvLOG6JxeycWsZN0wawNUTjyYjXaPRi0jTpdBIgJoa549z1/C/r62gW/ss/vadkxjdp1PUZYmIHDKFRpxt2l7Ofz21iLdXb+HcYUdwz38Op0NrzXkhIs2DQiOOZi3fxE3PLGF3RTX3fnEYXxndCzN1dotI86HQiIPyymrufTmfae+sY/CR7fn1V0dyTI46u0Wk+VFoHKJVm3Zw7RMLyf9kB5eP68st5wykZUZ61GWJiCSEQuMguTt/nbeBn/3jA9pkZvDIlNFMzM2JuiwRkYRSaByEkrIKbn02j1eWfcL4/l343y+PIKddVtRliYgknELjAP17zRa+/7dFbN65h9vPzeWKk/uRpiu7ReQwodCIUVV1Db+etYrfvrGaozq34bmrxjGsZ4eoyxIRSSqFRgw2bi3j+39bxHvrt3Hh8T35yeQhtGmpl05EDj9652vEP5Z8xG3P5YHDry4+lvOP7RF1SSIikVFoNKCsooo7py/jqQUFjOydza8vHkmvTq2jLktEJFIKjXps3FrGvN9fyRf3rObanFb0aNWKtBfV2S0iTcgRw+Cce+O+W4VGPXLat6R9VgsGdWpPhyyNGyUiUkuhUY+WGemc8YNHoi5DRCTlaHIHERGJmUJDRERiptAQEZGYKTRERCRmCg0REYmZQkNERGKm0BARkZgpNEREJGbm7lHXkDBmVgysj7qOOroAm6MuohGpXmOq1wepX2Oq1wepX2Oq1weHVuNR7t61vhXNOjRSjZktcPdRUdexP6leY6rXB6lfY6rXB6lfY6rXB4mrUc1TIiISM4WGiIjETKGRXA9GXUAMUr3GVK8PUr/GVK8PUr/GVK8PElSj+jRERCRmOtMQEZGYKTRERCRmCo0kMLNeZvaGmX1gZsvM7Pqoa6qPmaWb2UIz+0fUtdTHzLLN7Bkzyzez5WZ2UtQ11WVmN4R/36Vm9oSZZaVATVPNrMjMltZZ1snMZpjZqvBnxxSs8f7w77zEzJ43s+xUqq/Ouh+YmZtZlyhqq1NHvTWa2bXh67jMzH4ej2MpNJKjCviBuw8GTgSuNrPBEddUn+uB5VEXsR+/Al5x91xgBClUq5n1AK4DRrn7UCAduDjaqgCYBpy9z7JbgVnu3h+YFT6O0jQ+X+MMYKi7DwdWArclu6g6pvH5+jCzXsCZwIZkF1SPaexTo5lNBM4HRrj7EOCBeBxIoZEE7v6xu78f3t9B8GbXI9qqPsvMegJfAB6Kupb6mFkH4BTgYQB3r3D3kmir+pwMoJWZZQCtgY8irgd3nwts3Wfx+cCj4f1HgQuSWtQ+6qvR3V9z96rw4btAz6QX9mkt9b2GAL8EbgYi/zZRAzVeBdzr7nvCbYricSyFRpKZWR9gJPDvaCv5nP8j+A9QE3UhDegLFAOPhE1oD5lZm6iLquXuhQSf5DYAHwOl7v5atFU1qJu7fxze/wToFmUxMbgceDnqIuoys/OBQndfHHUt+zEAGG9m/zazOWY2Oh47VWgkkZm1BZ4Fvu/u26Oup5aZnQcUuft7UdeyHxnAccDv3X0ksIvom1X2CvsFzicIt+5AGzP7erRVNc6D79xH/km5IWb2Q4Lm3b9EXUstM2sN3A78OOpaGpEBdCJoEr8JeMrM7FB3qtBIEjNrQRAYf3H356KuZx/jgMlmtg54EjjNzB6PtqTPKQAK3L32DO0ZghBJFZOAte5e7O6VwHPA2IhrasgmMzsSIPwZl2aLeDOzKcB5wCWeWheUHU3w4WBx+H+mJ/C+mR0RaVWfVwA854F5BK0Ih9xhr9BIgjDdHwaWu/svoq5nX+5+m7v3dPc+BJ23r7t7Sn1KdvdPgI1mNjBcdDrwQYQl7WsDcKKZtQ7/3qeTQh31+5gOXBrevxR4McJa6mVmZxM0l05297Ko66nL3fPcPcfd+4T/ZwqA48J/o6nkBWAigJkNADKJw8i8Co3kGAd8g+AT/KLwdm7URTVB1wJ/MbMlwLHA3RHXs1d4BvQM8D6QR/B/K/KhJszsCeBfwEAzKzCzbwH3AmeY2SqCM6R7U7DG3wLtgBnh/5c/pFh9KaWBGqcC/cKv4T4JXBqPMzYNIyIiIjHTmYaIiMRMoSEiIjFTaIiISMwUGiIiEjOFhoiIxEyhIbIfZrYz/NnHzL4W533fvs/jd+K5f5FEUGiIxKYPcEChEQ5cuD+fCQ13T9UryEX2UmiIxOZegsHfFoXzZqSHcz7MD+d8+A6AmU0wszfNbDrhFetm9oKZvRfOaXBluOxeghFxF5nZX8JltWc1Fu57qZnlmdlX6ux7tn06p8hfascSMrN7LZivZYmZxWUIbJH6NPZJSEQCtwI3uvt5AOGbf6m7jzazlsDbZlY7qu1xBHNBrA0fX+7uW82sFTDfzJ5191vN7Bp3P7aeY32R4Ir3EQRjBc03s7nhupHAEIJh198GxpnZcuA/gVx39ygnLJLmT2caIgfnTOCbZraIYJj7zkD/cN28OoEBcJ2ZLSaYF6JXne0acjLwhLtXu/smYA5QO6z1PHcvcPcaYBFBs1kpUA48bGZfBFJqrCZpXhQaIgfHgGvd/djw1rfO/Bm79m5kNoFgfKeT3H0EsBA4lGlg99S5Xw1khJMVjSEY++o84JVD2L/Ifik0RGKzg2AAvVqvAleFQ95jZgMamBSqA7DN3cvMLJdgboNalbXP38ebwFfCfpOuBDMWzmuosHCelg7u/hJwA0GzlkhCqE9DJDZLgOqwmWkawXzlfQjmUTCCWQXrmzb1FeC7Yb/DCoImqloPAkvM7H13v6TO8ueBk4DFBBMk3ezun4ShU592wItmlkVwBvRfB/crijROo9yKiEjM1DwlIiIxU2iIiEjMFBoiIhIzhYaIiMRMoSEiIjFTaIiISMwUGiIiErP/D91FlKogTqSqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8T6q3NsDMv2"
      },
      "source": [
        "We see that the Value Iteration algorithm converged at 16th iteration. Hence the value of N is 16, i.e., the number of iterations it took to converge.\n",
        "\n",
        "After each iteration, I have computed the intermediate policy which is used to simulate episodes to check the learning of the agent. 20 episodes are simulated and the mean of returns observed is taken.\n",
        "\n",
        "We see that the learning curve is increase, i.e., the mean rewards is increasing through iterations. Hence, the agent is learning policies through iteraitons. We see that towards the end, the mean reward obtained is around 25."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRfSHjKEKkjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25da59c4-b6d5-477f-c646-e2f02c5293f7"
      },
      "source": [
        "# Agent's policy\n",
        "vi_agent.policy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9JUbu23DNT6"
      },
      "source": [
        "We see that the optimal policy obtained after training the agent using Value Iteration algorithm is indeed optimal. All the actions in every state are aigned towards the terminal state without falling into cliff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBZEeFDdKkxk"
      },
      "source": [
        "# DP Agent using Policy Iteration algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVoSkJIiew-2"
      },
      "source": [
        "class DPAgentPolicyIteration():\n",
        "    '''\n",
        "    Models Dynamic Programming agent which will be trianed using Policy Iteration algorithm\n",
        "    '''\n",
        "    def __init__(self, env, params):\n",
        "        '''\n",
        "        Method to initialize model\n",
        "\n",
        "        Input\n",
        "        -----\n",
        "        env     : Environment\n",
        "        params  : Parameters for training and simulation\n",
        "        '''\n",
        "        self.env = env\n",
        "        self.nS = env.observation_space.n\n",
        "        self.nA = env.action_space.n\n",
        "        self.params = params\n",
        "\n",
        "        self.V = np.random.rand(self.nS)\n",
        "        self.V[-1] = 0\n",
        "        self.policy = np.random.randint(self.nA, size=self.nS)\n",
        "        self.policy[-1] = 0\n",
        "\n",
        "    def policy_evaluation(self, policy):\n",
        "        '''\n",
        "        Method for policy evaluation given the policy\n",
        "\n",
        "        Input\n",
        "        -----\n",
        "        policy: Policy to be evaluated\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        V: State values corresponding to input policy\n",
        "        '''\n",
        "        V = np.random.rand(self.nS)\n",
        "        V[-1] = 0\n",
        "        iter = 0\n",
        "\n",
        "        while True and iter < 10000:\n",
        "            delta = 0\n",
        "            \n",
        "            for state in range(self.nS):\n",
        "                v = V[state]\n",
        "                val = 0\n",
        "\n",
        "                for tmp in self.env.P[state][policy[state]]:\n",
        "                    val += tmp[0] * (tmp[2] + self.params[\"discount\"] * V[tmp[1]])\n",
        "\n",
        "                V[state] = val\n",
        "                delta = max(delta, abs(V[state] - v))\n",
        "\n",
        "            if delta < self.params[\"tol\"]:\n",
        "                break\n",
        "\n",
        "            iter += 1\n",
        "            \n",
        "        return V\n",
        "\n",
        "    def policy_improvement(self, V, policy):\n",
        "        '''\n",
        "        Method for policy improvement\n",
        "\n",
        "        Input\n",
        "        -----\n",
        "        V       : State values\n",
        "        policy  : Input policy\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        policy          : Improved policy\n",
        "        policy_stable   : Status of the policy improvement (Boolean)\n",
        "        '''\n",
        "        policy_stable = True\n",
        "\n",
        "        for state in range(self.nS):\n",
        "            old_action = policy[state]\n",
        "            max_value = -100000\n",
        "            max_action = -1\n",
        "\n",
        "            for action in range(self.nA):\n",
        "                val = 0\n",
        "\n",
        "                for tmp in self.env.P[state][action]:\n",
        "                    val += tmp[0] * (tmp[2] + self.params[\"discount\"] * V[tmp[1]])\n",
        "\n",
        "                if val > max_value:\n",
        "                    max_value = val\n",
        "                    max_action = action\n",
        "\n",
        "            policy[state] = max_action\n",
        "\n",
        "            if old_action != policy[state]:\n",
        "                policy_stable = False\n",
        "\n",
        "        return policy, policy_stable\n",
        "\n",
        "    def policy_iteration(self):\n",
        "        '''\n",
        "        Method for policy iteration\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        V       : State values corresponding to optimal policy\n",
        "        policy  : Optimal policy\n",
        "        returns : List of returns of each iteration\n",
        "        '''\n",
        "        V = np.random.rand(self.nS)\n",
        "        V[-1] = 0\n",
        "        # policy = np.random.randint(self.nA, size=self.nS)\n",
        "        policy = np.zeros(nS)\n",
        "        policy[-1] = 0\n",
        "        \n",
        "        policy_stable = False\n",
        "        iter = 0\n",
        "        returns = []\n",
        "\n",
        "        while not policy_stable and iter < self.params[\"limit\"]:\n",
        "            # Policy Evaluation\n",
        "            V = self.policy_evaluation(policy)\n",
        "            self.V = V\n",
        "\n",
        "            # Policy Improvement\n",
        "            policy, policy_stable = self.policy_improvement(V, policy)\n",
        "            self.policy = policy\n",
        "\n",
        "            rets = []\n",
        "            for i in range(self.params[\"sim_N\"]):\n",
        "                rets.append(self.simulate())\n",
        "            returns.append(np.mean(rets))\n",
        "\n",
        "            iter += 1\n",
        "\n",
        "        print(\"Number of iterations taken for convergence:\", iter, \"\\n\")\n",
        "\n",
        "        return V, policy, returns\n",
        "\n",
        "    def train_agent(self, ):\n",
        "        '''\n",
        "        Method to train agent\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        returns: List of returns of each iteration of policy iteration\n",
        "        '''\n",
        "        self.V, self.policy, returns = self.policy_iteration()\n",
        "        return returns\n",
        "\n",
        "    def simulate(self):\n",
        "        '''\n",
        "        Method for simulation based on current policy\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        ret: Return of the simulation\n",
        "        '''\n",
        "        ret = 0\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        iter = 0\n",
        "\n",
        "        while not done and iter < self.params[\"max_iters\"]:\n",
        "            if random.uniform(0, 1) < 0.1:\n",
        "                action = self.env.action_space.sample()\n",
        "            else:\n",
        "                action = self.policy[state]\n",
        "\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            ret += reward\n",
        "\n",
        "            state = obs\n",
        "            iter += 1\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def get_nS(self):\n",
        "        '''\n",
        "        Getter method for number of states\n",
        "        '''\n",
        "        return self.nS\n",
        "\n",
        "    def get_nA(self):\n",
        "        '''\n",
        "        Getter method for number of actions\n",
        "        '''\n",
        "        return self.nA\n",
        "\n",
        "    def get_policy_state(self, state):\n",
        "        '''\n",
        "        Getter method for current policy of input state\n",
        "        '''\n",
        "        return self.policy[state]\n",
        "\n",
        "    def get_state_value(self, state):\n",
        "        '''\n",
        "        Getter method for state value of input state\n",
        "        '''\n",
        "        return self.V[state]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "tt8hKJTQiGK9",
        "outputId": "2583f12b-0324-4a28-e132-1392342752fd"
      },
      "source": [
        "pi_agent = DPAgentPolicyIteration(env, params)\n",
        "returns = pi_agent.train_agent()\n",
        "\n",
        "# Plot the returns vs iterations plot\n",
        "len1 = len(returns)\n",
        "x = np.arange(len1)+1\n",
        "plt.plot(x, returns)\n",
        "\n",
        "min_ret, max_ret, mean_ret = np.min(returns), np.max(returns), np.mean(returns)\n",
        "plt.plot(x, np.ones(len1)*min_ret, label=\"min\")\n",
        "plt.plot(x, np.ones(len1)*max_ret, label=\"max\")\n",
        "plt.plot(x, np.ones(len1)*mean_ret, label=\"mean\")\n",
        "plt.legend()\n",
        "plt.title(\"Rewards vs Iterations\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Rewards\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI9_P3O0DfhT"
      },
      "source": [
        "We see that the Policy Iteration algorithm converged at 14th iteration. Hence the value of N is 14, i.e., the number of iterations it took to converge.\n",
        "\n",
        "After each iteration, we have the intermediate policy obtained after policy improvement which is used to simulate episodes to check the learning of the agent. 20 episodes are simulated and the mean of returns observed is taken.\n",
        "\n",
        "We see that the learning curve is increase, i.e., the mean rewards is increasing through iterations. Hence, the agent is learning policies through iterations. We see that towards the end, the mean reward obtained is around 25."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkkRDqgsmeyU",
        "outputId": "7af2fbc8-f69c-46cf-fdc7-88979e5457b6"
      },
      "source": [
        "pi_agent.policy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixvU4b2NDgTx"
      },
      "source": [
        "We see that the optimal policy obtained after training the agent using Policy Iteration algorithm is indeed optimal. All the actions in every state are aigned towards the terminal state without falling into cliff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElBxG7mMKo1Q"
      },
      "source": [
        "# Confused Agent (random actions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIvLbMeOKqPp"
      },
      "source": [
        "class ConfusedAgent():\n",
        "    '''\n",
        "    Models confused agent which selects actions randomly\n",
        "    '''\n",
        "    def __init__(self, env, params):\n",
        "        '''\n",
        "        Method to initialize the model\n",
        "\n",
        "        Input\n",
        "        -----\n",
        "        env    : Environment\n",
        "        params : Parameters\n",
        "        '''\n",
        "        self.env = env\n",
        "        self.nS = env.observation_space.n\n",
        "        self.nA = env.action_space.n\n",
        "        self.params = params\n",
        "\n",
        "        self.policy = np.random.randint(self.nA, size=self.nS)\n",
        "\n",
        "    def simulate(self):\n",
        "        '''\n",
        "        Method for simulating episode\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        ret: Return of the simulation\n",
        "        '''\n",
        "        ret = 0\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        iter = 0\n",
        "\n",
        "        while not done and iter < self.params[\"max_iters\"]:\n",
        "            action = self.env.action_space.sample()\n",
        "\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            ret += reward\n",
        "\n",
        "            state = obs\n",
        "            iter += 1\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def get_policy(self):\n",
        "        '''\n",
        "        Getter method for policy\n",
        "        '''\n",
        "        return policy\n",
        "\n",
        "    def get_policy_state(self, state):\n",
        "        '''\n",
        "        Getter method for policy of input state\n",
        "        '''\n",
        "        return self.policy[state]\n",
        "\n",
        "    def get_nS(self):\n",
        "        '''\n",
        "        Getter method for number of states\n",
        "        '''\n",
        "        return self.nS\n",
        "\n",
        "    def get_nA(self):\n",
        "        '''\n",
        "        Getter method for number of actions\n",
        "        '''\n",
        "        return self.nA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjd1LjGQK5Bs"
      },
      "source": [
        "confused_agent = ConfusedAgent(env, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2hezbqwYvb4"
      },
      "source": [
        "# Comparing performance of agents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZaAg8txYu9I",
        "outputId": "3b90c770-3a41-49ae-efa1-9a304a2feb26"
      },
      "source": [
        "vi_rew = []\n",
        "pi_rew = []\n",
        "cf_rew = []\n",
        "\n",
        "# Simulations on VI trained agent\n",
        "for i in range(100):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rew = 0\n",
        "\n",
        "    while not done:\n",
        "        obs, reward, done, info = env.step(vi_agent.get_policy_state(state))\n",
        "        rew += reward\n",
        "        state = obs\n",
        "    \n",
        "    vi_rew.append(rew)\n",
        "\n",
        "# Simulations on PI trained agent\n",
        "for i in range(100):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rew = 0\n",
        "\n",
        "    while not done:\n",
        "        obs, reward, done, info = env.step(pi_agent.get_policy_state(state))\n",
        "        rew += reward\n",
        "        state = obs\n",
        "    \n",
        "    pi_rew.append(rew)\n",
        "\n",
        "# Simulations on confused agent\n",
        "for i in range(100):\n",
        "    cf_rew.append(confused_agent.simulate())\n",
        "\n",
        "\n",
        "print(\"Reward from VI trained agent:\", np.mean(vi_rew))\n",
        "print(\"Reward from PI trained agent:\", np.mean(pi_rew))\n",
        "print(\"Reward from confused agent  :\", np.mean(cf_rew))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward from VI trained agent: -13.0\n",
            "Reward from PI trained agent: -13.0\n",
            "Reward from confused agent  : -109.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8aYJD0WZgeJ"
      },
      "source": [
        "I have averaged the rewards obtained from 100 episodes generated from the trained agents. We can clearly see that agents trained using Value Iteration and Policy Iteration algorithms are doing better than the confused agent as the mean reward obtained by episode using confused agent is **-109.6**, whereas that of agents trained using VI and PI algorithms are **-13** each. This is the best reward obtained by following the optimal policy, i.e. traveling from start state to terminal state along the cliff. Hence, we see that VI and PI trained agents are doing perfectly well, whereas confused agent takes random steps due to which the convergence is slower and hence reward is lower. Hence, VI and PI trained agents are doing better than confused agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBJL_qyCcN_z"
      },
      "source": [
        "# Discount Factor Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgSkpCR_cRx5"
      },
      "source": [
        "all_gammas = [0, 0.1, 0.5, 0.7, 1]\n",
        "\n",
        "def print_policy(policy):\n",
        "    print(policy[:12])\n",
        "    print(policy[12:24])\n",
        "    print(policy[24:36])\n",
        "    print(policy[36:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahzuI0z3cX6G",
        "outputId": "d8f09378-d432-4566-f1cd-3ae293caa9e7"
      },
      "source": [
        "print(\"Agents trained using Value Iteration algorithm\\n\")\n",
        "\n",
        "for gam in all_gammas:\n",
        "    print(\"Discount factor (gamma):\", gam)\n",
        "\n",
        "    params1 = params\n",
        "    params1[\"discount\"] = gam\n",
        "\n",
        "    vi_agent = DPAgentValueIteration(env, params1)\n",
        "    _ = vi_agent.train_agent()\n",
        "\n",
        "    print_policy(vi_agent.policy)\n",
        "    print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agents trained using Value Iteration algorithm\n",
            "\n",
            "Discount factor (gamma): 0\n",
            "Number of iterations taken to converge: 2 \n",
            "\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "\n",
            "Discount factor (gamma): 0.1\n",
            "Number of iterations taken to converge: 5 \n",
            "\n",
            "[0. 0. 3. 0. 0. 0. 0. 3. 0. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 2.]\n",
            "[1. 3. 0. 1. 0. 0. 1. 1. 1. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "\n",
            "Discount factor (gamma): 0.5\n",
            "Number of iterations taken to converge: 12 \n",
            "\n",
            "[0. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "\n",
            "Discount factor (gamma): 0.7\n",
            "Number of iterations taken to converge: 16 \n",
            "\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "\n",
            "Discount factor (gamma): 1\n",
            "Number of iterations taken to converge: 16 \n",
            "\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "autSjEHafJeG",
        "outputId": "1d50dd0d-c366-4b1a-9ea3-45ee5cd4a9b5"
      },
      "source": [
        "print(\"Agents trained using Policy Iteration algorithm\\n\")\n",
        "\n",
        "for gam in all_gammas:\n",
        "    print(\"Discount factor (gamma):\", gam)\n",
        "\n",
        "    params1 = params\n",
        "    params1[\"discount\"] = gam\n",
        "\n",
        "    pi_agent = DPAgentPolicyIteration(env, params1)\n",
        "    _ = pi_agent.train_agent()\n",
        "\n",
        "    print_policy(pi_agent.policy)\n",
        "    print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agents trained using Policy Iteration algorithm\n",
            "\n",
            "Discount factor (gamma): 0\n",
            "Number of iterations taken for convergence: 1 \n",
            "\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "\n",
            "Discount factor (gamma): 0.1\n",
            "Number of iterations taken for convergence: 100 \n",
            "\n",
            "[0. 3. 1. 0. 0. 3. 3. 0. 3. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "\n",
            "Discount factor (gamma): 0.5\n",
            "Number of iterations taken for convergence: 12 \n",
            "\n",
            "[0. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "\n",
            "Discount factor (gamma): 0.7\n",
            "Number of iterations taken for convergence: 12 \n",
            "\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "\n",
            "Discount factor (gamma): 1\n",
            "Number of iterations taken for convergence: 11 \n",
            "\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rb5FFrtdT11"
      },
      "source": [
        "The above policies are obtained by training the agents using Value Iteration algorithm and Policy Iteration algorithm. We see that policies obtained by these two agents are almost similar for all discount factors.\n",
        "\n",
        "### Discount factor 0\n",
        "\n",
        "We see that all states have the policy (action) as 0 for gamma 0. This is the case for both agents trained by VI and PI. That means the agent hasn't learnt anything. The number of iterations taken to converge is also only **2/1** in VI/PI, which also shows that the agent didn't learn anything. This is because when gamma is 0, the agent becomes myopic (looks at only immediate rewards), due to which it didn't bother about future rewards and looked at current immediate rewards. But as we initialize state values with 0s, it converged in 1 step. Hence, discount factor os 0 will make the agent myopic and hence the convergence will not be guaranteed, because it doesn't care if the terminal state has to be reached.\n",
        "\n",
        "### Discount factor 0.1\n",
        "We see that when discount factor (gamma) is 0.1, some states have updated their policies (best actions) in both agents VI/PI. We can see that one third of right most states, all have policy (determinstic action) as optimal actions to be taken to reach terminal state. Right most staes have action as 2 (down) which is required to reach state 47. Similary all other states in one third of right most states have action as 1 (right), which is required to reach the last column state, which lead to terminal state 47. But we can also see that many other states have 3s and 0s, which are not optimal actions, mainly the left two thirds of states. But lower discount factor constrains the agent to finish quicker due to which it finds suboptimal actions which leads to other states than terminal states which are nearer to those states and which maximize the rewards. Hence, states which are farther from terminal states doesn't get updated with optimal policy. Only neared states are updated with optimal policy.\n",
        "\n",
        "### Discount factor 0.5\n",
        "For discount factor 0.5, we see that now almost all states have optimal policy, except for states 0, 1 and 12 which are the most distant states from the terminal state. This is the case for both agents trained using VI/PI algorithms. Hence, we see that as the discount factor is increase the states farther from terminal states are updated with optimal policy based on distance.\n",
        "\n",
        "### Discount factor 0.7\n",
        "For discount factor 0.7, we see that for both agents trained using VI and PI, all states have optmial policy, i.e., from any states, it takes the shortest path to reach the terminal states and hence maximizing the reward.\n",
        "\n",
        "### Discount factor 1\n",
        "For discount factor 1 also, we see that for both agents trained using VI and PI, all states have optimal policy. But we also see that Policy Iteration algorithm converged after 100 iterations (which is upper limit fed into the agent). However, we saw that the agent was stuck in policy evaluation stage, where it computes the state values corresponding to input policy. Discount factor 0 means the agent can take infinite time to finish the episode and hence, it was stuck in infinite loop, while evaluating a policy. Hence, discount factor 1 can have issues with convergence (infinite horizon or slow convergence).\n",
        "\n",
        "## Analysis\n",
        "Hence, we saw that discount factor has an impact on convergence, path of the agent from any state and optimal policy. We see that lower discount factor can have longer paths from distant states to terminal state (because distant states are updated with suboptimal policy), whereas for higher discount factors, all states have optimal policy and hence shortest path to terminal state from any state. However, keeping discount factor as 1 can lead to issues with convergence."
      ]
    }
  ]
}